\chapter{Properties of the two dimensional $\SL_2(F)$}\label{Ch5_PropertiesOfSLOverAlgClosedField}


\section{General Notation}

Throughout this paper, $F$ will denote an arbitrary algebraically closed field. 
The letter $p$ will be used to denote the characteristic of $F$. 
Recall that the definition of the characteristic of a field is:

\begin{definition}[Characteristic of a field]
    Let $F$ be a field, the characteristic of a field, denoted by $\textrm{char}(F) \in \N$, is the smallest natural number $p \in \N_0$ such that

    \[
    \underbrace{1 + \ldots + 1}_{p} = 0
    \]

    where in the case there is no such number then $p = 0$.
\end{definition}

\begin{example}
    $\Z /p\Z$ is a field of characteristic $\textrm{char}(\Z/p\Z) = p$ as $p \cdot 1 = 0$.
\end{example}

\begin{example}
    The field $\Q$ is a field with $\textrm{char}(\Q) = 0$ as $n \cdot 1 \ne 0$ for all $n \in \N \subset \Q$.
\end{example}

\begin{remark}[The characteristic is either prime or zero]
    The characteristic of a field is either a prime number or zero.
\end{remark}

Unless otherwise stated, the letters $\alpha, \beta, \gamma, \delta$ and $\sigma$ will denote elements of $F$; 
whereas $\delta$ and $\rho$ will denote elements of $F^\times$, where $F^\times$ are the invertible, or equivalently, non-zero elements of $F$.

\section{Subsets of $\SL_2(F)$}

In this chapter we make some useful observations about specific elements and subgroups of $\SL_2(F)$. 

First, we define the following elements of $\SL_2(F)$.

\subsubsection{Special matrices of $\SL_2(F)$}

\begin{definition}[The diagonal matrix of $SL_2(F)$]
\label{SpecialMatrices.d}
\lean{SpecialMatrices.d}
\leanok
    Given an element $\delta \in F^\times$ we define the diagonal matrix:
    \[
    d_\delta = \begin{bmatrix}
        \delta & 0\\
        0 & \delta^{-1}
    \end{bmatrix}
    \]
\end{definition}


\begin{remark}[Constructing a term of $\SL_2(F)$]
    To construct a term of $\SL_2(F)$ one has to bear in mind that the special linear group is defined to
    be a subtype of matrices with determinant one, thus, in the \textit{anonymous constructor}
    one has to provide:

    \begin{itemize}
        \item The term of type \texttt{Matrix (Fin 2) (Fin 2) F}.
        \item The proof term that proves that the matrix term of type \texttt{Matrix (Fin 2) (Fin 2) F} has determinant one.
    \end{itemize}
\end{remark}

\begin{definition}[The shear matrix of $SL_2(F)$]
\label{SpecialMatrices.s}
\lean{SpecialMatrices.s}
\leanok
    Given an element $\delta \in F$ we define the shear matrix:
    \[
    s_\sigma  = \begin{bmatrix}
    1 & 0\\
    \sigma & 1
    \end{bmatrix}
    \]
\end{definition}


\begin{definition}[Rotation by $\pi / 2$ radians matrix]
\label{SpecialMatrices.w}
\lean{SpecialMatrices.w}
\leanok
 We denote the matrix which corresponds to a rotation by $\pi / 2$ radians to be:
 \[
 w = \begin{bmatrix}
    0 & -1\\
    1 & 0
 \end{bmatrix}
 \]
\end{definition}


The matrices $d$, $s$ and $w$ satisfy the following relations:


\begin{lemma}[Closure of $D$ under multiplication]
\label{SpecialMatrices.d_mul_d_eq_d_mul}
\uses{SpecialMatrices.d}
\lean{SpecialMatrices.d_mul_d_eq_d_mul}
\leanok
For any $\delta, \rho \in F^\times$ we have that
\[
d_\delta d_\rho = d_{\delta\rho}
\]
\end{lemma}
\begin{proof}
\leanok
    We verify by matrix multiplication that indeed:

    \begin{equation*}
        d_\delta d_\rho = \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \begin{bmatrix} \rho & 0 \\ 0 & \rho^{-1} \end{bmatrix} = 
        \begin{bmatrix} \delta \rho & 0 \\ 0 & \delta^{-1} \rho^{-1} \end{bmatrix} = d_{\delta \rho}.
    \end{equation*}
\end{proof}


\begin{lemma}[Closure of $S$ under multiplication]
\label{SpecialMatrices.s_mul_s_eq_s_add}
\uses{SpecialMatrices.s}
\lean{SpecialMatrices.s_mul_s_eq_s_add}
\leanok
    For any $\sigma, \gamma \in F$ we have that
    \[
    s_\sigma s_\gamma = s_{\sigma + \gamma}.
    \]
\end{lemma}
\begin{proof}
\leanok
    We verify by matrix multiplication that indeed:
\begin{equation*}
    s_\sigma s_\gamma = \begin{bmatrix} 1 & 0 \\ \sigma & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ \gamma & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ \sigma + \gamma & 1 \end{bmatrix} = s_{\sigma + \gamma}.
\end{equation*}
\end{proof}


\begin{lemma}
    \label{SpecialMatrices.s_pow_eq_s_mul}
    \uses{SpecialMatrices.s}
    \lean{SpecialMatrices.s_pow_eq_s_mul}
    \leanok
    For any $\sigma \in F$ and for any $n \in \N$, we have that $s_\sigma^n = s_{n \cdot \sigma}$
\end{lemma}
\begin{proof}
\uses{SpecialMatrices.s_mul_s_eq_s_add}
\leanok
    We prove this by induction, indeed for $n= 0$ the identity holds trivially.

    Suppose $s_\sigma^n = \begin{bmatrix}
        1 & 0\\
        n \cdot \sigma & 0\end{bmatrix}$ then consider $s_\sigma^{(n + 1)}$. Since 

        \[
        s_\sigma^{(n + 1)} = s_\sigma^n s_\sigma = s_{n \cdot \sigma} s_\sigma = s_{(n + 1)\sigma}
        \]
\end{proof}


\begin{lemma}[Order of nontrivial $s_\sigma$ ]
    \label{SpecialMatrices.order_s_eq_char}
    \uses{SpecialMatrices.s}
    \lean{SpecialMatrices.order_s_eq_char}
    \leanok
    The order of $s_\sigma$ for any $\sigma \ne 0$ is $\textrm{char}(F)$
    \end{lemma}
    
    \begin{proof}
    \uses{SpecialMatrices.s_pow_eq_s_mul}
    \leanok
    Let $p$ denote the characteristic of the field, and let $\sigma \in F$, by \ref{SpecialMatrices.s_pow_eq_s_mul} we know that for any $s_\sigma^p = s_{p \cdot \sigma}$. 
    Since $p$ is the characteristic of the field, we have that $p \cdot \sigma = 0$, and so $s_{p \cdot \sigma} = s_0 = I$
    \end{proof}


\begin{lemma}
\label{SpecialMatrices.d_mul_s_mul_d_inv_eq_s}
\uses{SpecialMatrices.d, SpecialMatrices.s}
\lean{SpecialMatrices.d_mul_s_mul_d_inv_eq_s}
\leanok
    We have that for all $\delta \in F^\times$ and $\sigma \in F$
    \[
    d_\delta s_\sigma d^{-1}_\delta = s_{\sigma \delta^{-2}}.
    \]
\end{lemma}
\begin{proof}
\leanok
    We verify by matrix multiplication that indeed:

    \begin{equation*}
        d_\delta s_\sigma d^{-1}_\delta = \! \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \begin{bmatrix} 1 & 0 \\ \sigma & 1 \end{bmatrix} \begin{bmatrix} \delta^{-1} & 0 \\ 0 & \delta \end{bmatrix} = \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \! \begin{bmatrix} \delta^{-1} & 0 \\ \sigma \delta^{-1} & \delta \end{bmatrix} \! = \! \begin{bmatrix} 1 & 0 \\ \sigma \delta^{-2} & 1 \end{bmatrix} \! = s_{\sigma \delta^{-2}}.
    \end{equation*}
\end{proof}




\begin{lemma}
\label{SpecialMatrices.w_mul_d_eq_d_inv_w}
\uses{SpecialMatrices.d, SpecialMatrices.w}
\lean{SpecialMatrices.w_mul_d_eq_d_inv_w}
\leanok
For any $\delta \in F^\times$ we have:
\[ 
w d_\delta w^{-1} = d^{-1}_\delta.
\]
\end{lemma}
\begin{proof} 
\leanok
We verify by matrix multiplication that indeed
\begin{align*}
w d_\delta w^{-1} &= \begin{bmatrix} 0 & 1 \\ - 1 & 0 \end{bmatrix} \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \begin{bmatrix} 0 & - 1 \\ 1 & 0 \end{bmatrix}\\
&=  \begin{bmatrix} 0 & 1 \\ - 1 & 0 \end{bmatrix} \begin{bmatrix} 0 & - \delta \\ \delta^{-1} & 0 \end{bmatrix}\\
&= \! \begin{bmatrix} \delta^{-1} & 0 \\ 0 & \delta \end{bmatrix} \!= d^{-1}_\delta. 
\end{align*}
\end{proof}


We can now express familiar kinds of matrices of $\SL_2(F)$ in terms of these three matrices:

First we note the following observations:
\begin{corollary}
    \label{det_eq_mul_diag_of_lower_triangular}
    \lean{det_eq_mul_diag_of_lower_triangular}
    \leanok
    The determinant of a $2 \times 2$ lower triangular matrix, $M$, is the product of the diagonal entries $\det(M) = M_{11} M_{22}$.
\end{corollary}
\begin{proof}
\leanok
We use the $2 \times 2$ determinant formula.
\end{proof}


\begin{corollary}
    \label{SpecialLinearGroup.fin_two_diagonal_iff}
    \lean{SpecialLinearGroup.fin_two_diagonal_iff}
    \leanok
    A $2 \times 2$ matrix of $\SL_2(F)$, $x$ is a diagonal matrix if and only if $x = d_\delta$ for some $\delta \in F^\times$.
\end{corollary}
\begin{proof}
\uses{det_eq_mul_diag_of_lower_triangular}
\leanok
    Since $x$ is diagonal and belongs to the special linear group, the determinant is $x_{11} x_{22} = 1$ which shows $x_{11} = x_{22}^{-1}$, as required.
\end{proof}


\begin{corollary}
    \label{SpecialLinearGroup.fin_two_shear_iff}
    \uses{SpecialMatrices.s, det_eq_mul_diag_of_lower_triangular}
    \lean{SpecialLinearGroup.fin_two_shear_iff}
    \leanok
    A matrix of $\SL_2(F)$, $x$ is a shear matrix, that is of the form $\begin{bmatrix}
        \alpha & 0\\
        \sigma & \alpha
    \end{bmatrix}$ if and only if either $x = s_\sigma$ or $x = - s_\sigma$ for some $\sigma \in F$.
\end{corollary}
\begin{proof}
\leanok
Again using the formula for the determinant of a $2 \times 2$ matrix to show that indeed if $x$ is a shear matrix in the special linear group 
then $\alpha^2 = 1$ which shows $\alpha = \pm 1$, as required.
\end{proof}


\begin{corollary}
    \label{SpecialLinearGroup.fin_two_antidiagonal_iff}
    \lean{SpecialLinearGroup.fin_two_antidiagonal_iff}
    \leanok
    A matrix $A \in \SL_2(F)$ is anti-diagonal, that is of the form $\begin{bmatrix}
        0 & \beta\\
        \gamma & 0
    \end{bmatrix}$ if and only if $A = d_\delta w$
\end{corollary}
\begin{proof}
\leanok
This is shown by direct computation, we observe that $w$ flips the rows and changes the sign of one the flipped rows to account for the determinant needing to be equal to one.
\end{proof}


From these relations we can now single out the following subgroups of $\SL_2(F)$.

\subsubsection{Special subgroups of $\SL_2(F)$}

\begin{definition}[The subgroup of diagonal matrices]
\label{SpecialSubgroups.D}
\lean{SpecialSubgroups.D}
\leanok
    The set of diagonal matrices with matrix multiplication is a subgroup of $\SL_2(F)$: 
    \[
    D = \{d_\delta \; | \; \delta \in F^\times \} = \left\{ \begin{bmatrix}\delta & 0\\ 0 & \delta^{-1}\end{bmatrix} \; | \; \delta \in F^\times \right\}
    \]
\end{definition}


\begin{definition}[The subgroup of shear matrices]
\label{SpecialSubgroups.S}
\lean{SpecialSubgroups.S}
\leanok
    The set of shear matrices with matrix multiplication is a subgroup of $\SL_2(F)$:
    \[
    S = \{s_\sigma \; | \sigma \in F\} = \left\{\begin{bmatrix}1 & 0\\ \sigma & 1\end{bmatrix} \; | \; \sigma \in F \right\}
    \]
\end{definition}


\begin{definition}[The subgroup of lower triangular matrices]
\label{SpecialSubgroups.L}
\lean{SpecialSubgroups.L}
\leanok
    The set of lower triangular matrices (see below) with matrix multiplication is a subgroup of $\SL_2(F)$
    \[
    L = DS
    \]
    where $DS = \{d_\delta s_\sigma \; | \; \delta \in F^\times \text{ and } \sigma \in F \}$ is the pointwise product of $D$ and $S$.
\end{definition}


\begin{definition}[The subgroup of containing diagonal and antidigonal matrices]
    \label{SpecialSubgroups.DW}
    \lean{SpecialSubgroups.DW}
    \leanok
    The set of all diagonal and anti-diagonal matrices with matrix multiplication is a subgroup of $\SL_2(F)$

    \begin{equation}\label{antidiag} DW = \langle D, w\rangle  = \{d_\delta \} \cup \{ d_\delta w \} 
        % =  \left\{  \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \right\} 
        % = \left\{ \begin{bmatrix} 0 & \delta \\ -\delta^{-1} & 0 \end{bmatrix}  \right\}. 
    \end{equation}
\end{definition}


\begin{remark}
    It is possible to have specified the subgroup $DW$ in \ref{SpecialSubgroups.DW} as the supremum $D \sqcup \langle w \rangle$
    but then it would require some additional work to show that the underlying set is indeed $D \cup Dw$.
\end{remark}


\begin{corollary}
    \label{mem_L_iff_lower_triangular}
    \uses{SpecialMatrices.d, SpecialMatrices.s}
    \lean{mem_L_iff_lower_triangular}
    \leanok
    The subgroup $L \le \SL_2(F)$ is the subgroup of $2 \times 2$ lower triangular matrices with determinant one, $L =\left\{\begin{bmatrix}
    \alpha & 0\\
    \gamma & \delta
    \end{bmatrix} \; | \; \alpha, \gamma, \delta \in F \text{ and } \alpha \delta = 1 \right\}$.
\end{corollary}
\begin{proof}
\leanok
    Observe that for every $l \in L$ there is some $\delta \in F^\times$ and $\sigma \in F$ such that $l  = d_\delta s_\sigma = \begin{bmatrix}
        \delta & 0\\
        \sigma * \delta^{-1} & \delta^{-1}
    \end{bmatrix}$ which is lower triangular. 
    
    Furthermore, for every lower triangular matrix $L = \begin{bmatrix}
        a & 0\\
        b & c
    \end{bmatrix}$ 
    
    Setting $\delta = a \in F^\times$ as $a d = 1$ and setting $\sigma = a c$

    indeed yields the equality

    \[
    d_\delta s_\sigma = \begin{bmatrix}
        a & 0\\
        c & d
    \end{bmatrix}
    \]

    Thus $L = D S$ is the set of lower triangular matrices.
\end{proof}


\begin{remark}
    To define the subgroups $D$, $S$ and $L$ in Lean. 
    
    One has to:
    
    \begin{enumerate}
        \item Specify what the underlying set is, what is called the \texttt{carrier}.
        \item Prove that the set is closed under multiplication, that is, provide a proof term to the field \texttt{mul\textunderscore mem'}.
        \item Prove that the set contains the identity element of the group, that is, provide a proof term to the field \texttt{one\textunderscore mem'}.
        \item Show that the group is closed under the inversion operator $(-)^{-1}$, \texttt{inv\textunderscore mem'}.
    \end{enumerate}

    Once these four fields have been filled in, one has succesfully defined a subgroup in Lean.
\end{remark}


\begin{remark}
    Despite the definition of $L$ as being $D S$, some work has to be shown that indeed $DS = D \sqcup S$.
    
    If either $D$ or $S$ were normal in $\SL_2(F)$, this fact would be immediate as we would be able to use \texttt{mul\_normal} or \texttt{normal\_mul}:
    
   
    

    However, given neither $D$ or $S$ are normal in $\SL_2(F)$ slightly more work is needed to show this.
    
    It is interesting how Lean really forces either increased understanding or increased frustration.
\end{remark}

These elements and subgroups are fundamental to this paper and the notation will be used throughout.

\begin{definition}[$(D, \cdot) \cong (F^\times, \cdot)$]
\label{SpecialSubgroups.D_iso_units}
\uses{SpecialSubgroups.D, SpecialMatrices.d_mul_d_eq_d_mul}
\lean{SpecialSubgroups.D_iso_units}
\leanok
The map $\psi : F^\times \overset{\sim}{\rightarrow} D$ defined by $\delta \mapsto d_\delta$ defines a group isomorphism.
\end{definition}

\begin{proof}
    \leanok
    The function $\psi: F^\times \rightarrow D$ defined by $\psi(\delta) = d_\delta$ is a homomorphism between the group $F^\times$ under normal multiplication and $D$ under normal matrix multiplication:
\begin{align*} 
  \psi(\delta \rho) = d_{\delta \rho} =  d_\delta d_\rho = \psi(\delta) \psi(\rho). 
\end{align*}
Observe that $\psi$ is trivially injective and surjective and thus an isomorphism. So $D\cong F^\times$ and $D$ is a subgroup of $L$.\\
\end{proof}




\begin{definition}[ $(S, \cdot) \cong (F, +)$ ]
\label{SpecialSubgroups.S_iso_F}
\uses{SpecialSubgroups.S}
\lean{SpecialSubgroups.S_iso_F}
\leanok
    The map $\phi : F \overset{\sim}{\rightarrow} S$ defined by $\sigma \mapsto s_\sigma$ defines a group isomorphism.
\end{definition}

\begin{proof}
\uses{SpecialMatrices.s_mul_s_eq_s_add}
\leanok
     The function $\phi: F \rightarrow T$ defined by $\phi(\sigma) = s_\sigma$ is a homomorphism between the group $F$ under addition and $S$ under normal matrix multiplication:
\begin{align*} \phi(\sigma + \gamma) = s_{\sigma + \gamma} = s_\sigma s_\gamma = \phi(\sigma) \phi(\gamma).
\end{align*}
It is clear that $\phi$ is injective and surjective and thus an isomorphism. So $ S \cong F$ and $S$ is a subgroup of $L$. \\
\end{proof}


\begin{remark}[Multiplicative]
    Putting the keyword \texttt{Multiplicative} in front a structure which carries an additive structure
    it creates a copy of the additive structure and carries it over to be defined as a multiplicative structure
    on the type. 
\end{remark}


\begin{lemma}
\label{SpecialSubgroups.normal_S_subgroupOf_L}
\lean{SpecialSubgroups.normal_S_subgroupOf_L}
\leanok
$S$ is a normal subgroup of $L$
\end{lemma}
\begin{proof}
    \leanok
    Let $s_\gamma$ and $d_\delta s_\sigma$ be arbitrary elements of $S$ and $L$ respectively. Conjugating $s_\gamma$ by $d_\delta s_\sigma$ gives,
\begin{align*} (d_\delta s_\sigma) s_\gamma (d_\delta s_\sigma)^{-1} &= (d_\delta s_\sigma) s_\gamma (s^{-1}_\sigma d^{-1}_\delta) \\[1.5ex]
&=
d_\delta (s_\sigma s_\gamma s_{-\sigma}) d^{-1}_\delta \qquad \tag{since $s^{-1}_\sigma=s_{-\sigma}$} \\[1.5ex] 
&=
d_\delta s_\gamma d^{-1}_\delta \\[1.5ex] 
&= s_{\gamma \delta^{-2}} \in S. 
\end{align*}
Since $s_\gamma$ was chosen arbitrarily from $\SL_2(F)$we have ($d_\delta s_\sigma) S (d_\delta s_\sigma)^{-1} = S$ and since $d_\delta s_\sigma$ was chosen arbitrarily from $L$, we have that $S \vartriangleleft L$. \\
\end{proof}


\begin{remark}[Subgroups of subgroups in Lean]
    \label{lattice}
    In Lean, $S$ is considered to be a subgroup of $\SL_2(F)$, yet it it is also a subgroup of $L$. 
    
    It is fairly easy to see that $S \not\lhd \SL_2(F)$, so when we say that $S \lhd L$, 
    we are implicitly restricting $S$ to be a subset of $L$ and thus we are actually thinking about the subgroup $S \cap L$,
    but in fact this does not change anything because $S = S \sqcap L$ as $S \le \SL_2(F)$.

    Informally we do not think twice about this, but when formalising this we do need to be clear which is the ambient group for $S$
    to be normal and for $S$ to be normal in an ambient group, it must be considered to be a subgroup of $L$, rather than $\SL_2(F)$.
    
    So this is why the informal statement corresponds to the formal statement:

    

    This example highlights how as useful as it is that Lean keeps track of what the ambient groups are, it can be tedious to change the perspective from which we view the object, where in this case we restricted
    a subgroup to be a subgroup of another subgroup that contains it. One of the challenges of Lean is becoming comfortable with these \textit{coercion}.
    
    On the positive side, the automation Lean offers, that is, the tactics and the unification algorithm (the algorithm which allows you to substitute equal terms when say you use the \texttt{rw} tactic) are continually being refined, 
    and it is increasingly able to do a lot of this bookkeeping on without human aid.
\end{remark}


\begin{lemma}
\label{SpecialSubgroups.D_join_S_quot_S_subgroupOf_D_join_S_mulEquiv_D_subgroupOf_D_join_S}
\uses{SpecialSubgroups.D, SpecialSubgroups.S}
\lean{SpecialSubgroups.D_join_S_quot_S_subgroupOf_D_join_S_mulEquiv_D_subgroupOf_D_join_S}
\leanok
    $L / S \cong D$.
\end{lemma}
\begin{proof} 
\uses{SpecialSubgroups.normal_S_subgroupOf_L}
\leanok
The function $\pi: L \rightarrow D$ defined by $\pi(d_\delta s_\sigma) = d_\delta$ is a homomorphism between $L$ under normal matrix multiplication and $D$ under normal matrix multiplication:
\begin{align*} \pi(d_\delta s_\sigma d_\rho s_\gamma) &= \pi(d_\delta d_\rho s_\sigma s_\gamma) \tag{where $\sigma = \sigma \rho^{2}$}
\\ &= d_\delta d_\rho
\\ &= \pi(d_\delta s_\sigma)\pi(d_\rho s_\gamma).
\end{align*}

We see that $\pi$ is trivially surjective and has kernel
\begin{align*}  \ker(\pi) &= \{ d_\delta s_\sigma \in L : \pi(d_\delta s_\sigma) = I_{\SL_2(F)}\} = S.
\end{align*}
Thus by the First Isomorphism Theorem,
\begin{align*} L / \ker(\pi) &\cong \text{Im}(\pi), \\
L / &\cong D.
\end{align*}
\end{proof}



\begin{remark}
Interestingly, this proof was quite hard to formalise for reasons I will expand on below, but first let me introduce some ideas.

There are two complete lattice structures at play here:
\begin{enumerate}
  \item One where the top element is $\top = \SL_2(F)$
  \item Another, where the top element is $\top = D \sqcup S$, this lattice is a sublattice of the first one.
\end{enumerate}  

The second sublattice is crucial because we need $S$ to be normal in an ambient group, and clearly $S \not\lhd \SL_2(F)$; therefore when restricting $S$ to begin a
subgroup of $D \sqcup S = L$. 

Given $S$ is a subgroup of $D \sqcup S = L$ since $S \le S\sqcup D = L$ and by \ref{SpecialSubgroups.normal_S_subgroupOf_L}
we know $S$ is normal in $L$.

We can then use then define the desired isomorphism by theorem
\texttt{QuotientGroup.quotientInfEquivProdNormalQuotient} which corresponds to the statement:



Which is in fact the second isomorphism theorem! Not the first isomorphism theorem! 

Which contrasts to how the statement was proved informally, where in for this particular theorem,
\texttt{QuotientGroup.quotientInfEquivProdNormalQuotient}, \texttt{H} is specialized to:




And \texttt{N} is specialized to:



Recall that within Lean, \texttt{F} denotes the base field for $\SL_2(F)$, $D$ and $S$.

Written informally, it then corresponds to the desired statement

\[
D \cong \frac{D}{\bot} = \frac{D}{S \sqcap D} \cong \frac{D \sqcup S}{S} = \frac{L}{S}
\]
\end{remark}

\section{The Center of $\SL_2(F)$}

\begin{definition}
% \lean{Subgroup.center}
% \leanok
The \textbf{center} $Z(G)$ of a group $G$ is the set of elements of  $G$ that commute with every element of $G$.
\begin {equation*} Z(G) = \{ z \in G : \forall g \in G, \hspace{6pt} gz=zg \}. \end{equation*}
It is an immediate observation that $Z(G)$ is a normal subgroup of $G$, 
since for each $z \in Z$, $gzg^{-1} = gg^{-1}z = z$, $\forall g \in G$. It's also clear that a group is abelian if and only if $Z(G)=G$.
\end{definition}

\begin{definition}
\label{SpecialSubgroups.Z}
\lean{SpecialSubgroups.Z}
\leanok
    Let $R$ be a commutative ring and define $Z$ to be the subgroup generated by $- I \in \SL_2(R)$
\end{definition}

\begin{remark}[Z as the subgroup closure of $\{-I\}$]
    Observe that the subgroup generated by an element $g \in G$, $\langle g \rangle$, 
    can be thought of more generally within any lattice (such as the lattice on modules) as the closure of a singleton set ${g}$. 
    
    Therefore, the subgroup generated by $-I$ is equal to
    
    \[\langle -I \rangle = \overline{\{-1\}} = \inf \{ K \le G \; | \; \{-1\} \subseteq K \}.\]

    When taking the closure of a singleton within the subgroup lattice; 
    the closure corresponds to taking the powers of the element in the singleton $\{g\}$,
    which is what is typically understood as the subgroup generated by $g$.

    The way $Z$ is defined in Lean is thus:

   
\end{remark}

\begin{corollary}
\label{SpecialSubgroups.closure_neg_one_eq}
\lean{SpecialSubgroups.closure_neg_one_eq}
\leanok
The subgroup closure of the singleton $\{-I\}$, or equivalently, the subgroup generated by $-I$ equals $\overline{\{-I\}} = \{I, -I\}$
\end{corollary}
\begin{proof}
\leanok
Since $-1^2 = 1$, we have that $-I^2 = I$ and thus the result follows.
\end{proof}



\begin{lemma}
\label{SpecialSubgroups.center_SL2_eq_Z}
\uses{SpecialSubgroups.Z}% Matrix.SpecialLinearGroup.mem_center_iff}
\lean{SpecialSubgroups.center_SL2_eq_Z}
\leanok
The center $Z(\SL_2(F)) = \langle - I_{\SL_2(F)}\rangle = Z$.
\end{lemma}
\begin{proof} 
\leanok
    Take an arbitrary element $x=\begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \in \SL_2(F)$and  an arbitrary element $z = \begin{bmatrix} z_1 & z_2 \\ z_3 & z_4 \end{bmatrix} \in Z$ and consider their product:

\begin{align}\label{myeq1} zx = \begin{bmatrix} z_1 & z_2 \\ z_3 & z_4 \end{bmatrix} \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} &= \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} z_1 & z_2 \\ z_3 & z_4 \end{bmatrix} = xz, \nonumber \\[1.5ex]
\begin{bmatrix} z_1 \alpha + z_2 \gamma & z_1 \beta + z_2 \delta \\ z_3 \alpha + z_4 \gamma & z_3 \beta + z_4 \delta \end{bmatrix} &= \begin{bmatrix} z_1 \alpha + z_3 \beta & z_2 \alpha + z_4 \beta \\ z_1 \gamma + z_3 \delta & z_2 \gamma + z_4 \delta \end{bmatrix}.
\end{align}

\noindent Equating either the top left or bottom right entries, we see that $z_2 \gamma = z_3 \beta$. Since $\beta$ and $\gamma$ can take any values in $F$, for equality to always hold we must have $z_2 = 0 = z_3$. Hence equation (\ref{myeq1}) simplifies to

\begin{equation*} \begin{bmatrix} z_1 \alpha & z_1 \beta \\ z_4 \gamma & z_4 \delta \end{bmatrix} = \begin{bmatrix} z_1 \alpha & z_4 \beta \\ z_1 \gamma & z_4 \delta \end{bmatrix}. \end{equation*}

Thus 
\begin{equation*} 
    z_1 = z_4 \qquad  \text{and} \qquad z =  
    \begin{bmatrix} z_1 & 0 \\ 0 & z_1 \end{bmatrix}. 
\end{equation*}
Since we are working in the special linear group, det$(z)=1$, thus $z_1 = \pm 1$ and $Z = \langle - I_{\SL_2(F)}\rangle$ as required. Observe that this is a cyclic group of order 2 except in the case of $p = 2$ where $- I_{\SL_2(F)} = I_{\SL_2(F)}$. \\
\end{proof}


Following this result, for ease of notation, $Z(\SL_2(F))$ will be denoted simply by $Z$ throughout the rest of this blueprint.

\begin{lemma}
\label{SpecialSubgroups.exists_unique_orderOf_eq_two}
\lean{SpecialSubgroups.exists_unique_orderOf_eq_two}
\leanok
    If $p\neq 2$, then $\SL_2(F)$ contains a unique element of order 2. \\
\end{lemma}
\begin{proof}
\leanok
Consider an arbitrary element $x \in \SL_2(F)$with order 2. That is $x^2 = I_{\SL_2(F)}$, $x \neq I_{\SL_2(F)}$and thus $x=x^{-1}$.
\begin{equation*} 
    x = \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} = \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix}^{-1} = \begin{bmatrix} \delta & - \beta \\ - \gamma & \alpha \end{bmatrix}.
\end{equation*}
\noindent Thus $\alpha = \delta$, $\beta = - \beta \Rightarrow 2\beta = 0$ and $\gamma = - \gamma \Rightarrow 2\gamma = 0$. In the case of $p \neq 2$ this gives $\beta = 0 = \gamma$. So
\begin{equation*} 
    x = \begin{bmatrix} \alpha & 0 \\ 0 & \alpha \end{bmatrix}.
\end{equation*}
\noindent Also $\alpha^2 = 1$ since $x \in$ $\SL_2(F)$, so $\alpha = \pm 1$. For $x$ to have order 2, we must have $\alpha = - 1$. Hence there is a unique element of order 2, namely $- I_{\SL_2(F)}$.
\end{proof}


\begin{lemma}
    \label{SpecialSubgroups.card_Z_eq_two_of_two_ne_zero}
    \lean{SpecialSubgroups.card_Z_eq_two_of_two_ne_zero}
    \leanok
    If $\textrm{char}(F) \ne 2$ then $|Z| = 2$.
\end{lemma}
\begin{proof}
\leanok
    If $\textrm{char}(F) \ne 2$ then $1 \ne -1$ as $2 \ne 0$ therefore, $I \ne -I$ which shows that $Z = \{I , -I\}$ contains two distinct elements.
\end{proof}


\begin{lemma}
    \label{SpecialSubgroups.card_Z_eq_one_of_two_eq_zero}
    \lean{SpecialSubgroups.card_Z_eq_one_of_two_eq_zero}
    \leanok
    If $\textrm{char}(F) = 2$ then $|Z| = 1$. 
\end{lemma}
\begin{proof}
\leanok
    If $\textrm{char}(F) = 2$ then $1 = -1$ as $2 = 0$ therefore, $I = -I$ which shows that $Z = \{I , -I\} = \{I\}$ only contains one element.
\end{proof}


\begin{lemma}[$Z$ is cyclic]
    \label{SpecialSubgroups.IsCyclic_Z}
    \lean{SpecialSubgroups.IsCyclic_Z}
    \leanok
\end{lemma}
\begin{proof}
\leanok
    By construction, $Z = \overline{\{-I\}} = \{-I^k \; | \; k \in \Z \} = \langle -I \rangle$, therefore $Z$ is generated by a single element and is thus cyclic.
\end{proof}


\begin{remark}[Typeclass instances]
    Observe that instead of telling Lean that \texttt{Is\_Cyclic\_Z} is a \texttt{theorem} we declare it to be
    an \texttt{instance} since we would like Lean to look for this fact whenever it requires it for a
    theorem that may require the assumption that $Z$ is a commutative subgroup.
\end{remark}

In the next chapter it will be useful to record the interactions between $S$ and $Z$. 
For instance we define the following subgroup

\begin{definition}
    \label{SpecialSubgroups.SZ}
    \uses{SpecialSubgroups.S, SpecialSubgroups.Z, SpecialMatrices.s_mul_s_eq_s_add}
    \lean{SpecialSubgroups.SZ}
    \leanok
    We define the subgroup $SZ$ to be the subgroup with the underlying set $S \cup -S$, or equivalently the pointwise product $SZ$.
\end{definition}



\begin{corollary}
\label{SpecialSubgroups.S_mul_Z_subset_SZ}
\uses{SpecialSubgroups.SZ}
\lean{SpecialSubgroups.S_mul_Z_subset_SZ}
$SZ = S \cup -S$
\leanok
\end{corollary}
\begin{proof}
\leanok
    By construction an element of $SZ$ is either of the form $s_\sigma  I = s_\sigma \in S$ or $s_\sigma -I = -s_\sigma\in -S$. The reverse subset inclusion is very similar.
\end{proof}


\begin{lemma}
    \label{SpecialSubgroups.S_join_Z_eq_SZ}
    \uses{SpecialSubgroups.Z, SpecialSubgroups.S, SpecialSubgroups.SZ}
    \lean{SpecialSubgroups.S_join_Z_eq_SZ}
    \leanok
    The join of subgroups satisfies $S \sqcup Z = SZ$
\end{lemma}
\begin{proof}
\uses{SpecialSubgroups.closure_neg_one_eq, SpecialSubgroups.S_mul_Z_subset_SZ}
\leanok
We show that $S \sqcup Z = SZ$ by antisymmetry, that is, we show both that
\begin{itemize}
    
    \item $S \sqcup Z \subseteq SZ$
    
    Let $x \in S \sqcup Z$, then if $x$ is in the subgroup closure then if $K$ is a subgroup whose underlying set contains $SZ$ then $x$ is in $K$,
    but since $SZ$ was shown to be a subgroup in \ref{SpecialSubgroups.SZ} we can conclude that $x \in SZ$ and thus $x = s_\sigma z$ for some $\sigma \in F$.
    
    \item $SZ \subseteq S \sqcup Z$
    
    Let $s_\sigma z \in SZ$ then we must show that $s_\sigma z$ is the subgroup closure of $S$ and $Z$ but since the subgroup closure 
    must at least contain the pointwise product whose underlying set is equal to the union $S \cup -S$, we are done.
\end{itemize}
\end{proof}


\section{Conjugacy of the Elements of $\SL_2(F)$}

\subsubsection{Classification of elements of $\SL_2(F)$ up to conjugation}

\begin{lemma}[Upper triangularizability criteria]
\label{isConj_upper_triangular_iff}
\lean{isConj_upper_triagnular_iff}
\leanok
    A matrix $M \in\textrm{Mat}(2; F)$ is triangularizable if and only if there exists an invertible matrix $C \in \GL_2(F)$ such that the bottom left entry
    $C M C^{-1}_{21} = 0$.
\end{lemma}

\begin{proof}
\leanok
    Given a matrix $U$ is in upper triangular form if and only if
    \[
    U = \begin{bmatrix}
    a & b\\
    0 & d
    \end{bmatrix}
    \]
    
    that is, the bottom left entry is zero. It then follows that $M$ is triangularizable if and only if
    there exists a $C \in \GL_2(F)$ such that $C M C^{-1}$ is in upper triangular form, that is, the bottom left entry of $C M C^{-1}$ is zero. 
\end{proof}


\begin{lemma}[Upper triangularizability of a $2 \times 2$ matrix over an algebraically closed field]
\label{isTriangularizable_of_algClosed}
\lean{isTriangularizable_of_algClosed}
\leanok
    When $F$ is an algebraically closed field, 
    for any $M \in \textrm{Mat}(2; F)$ there exists an invertible matrix $C \in \SL_2(F) \le \GL_2(F)$ such that $C M C^{-1} = U$ where
    \[
    U = \begin{bmatrix}
        a & b\\
        0 & d
    \end{bmatrix}\] for some $a, b, d \in F$.
\end{lemma}
\begin{proof}
    \uses{isConj_upper_triangular_iff}
    \leanok
We prove this by direct computation. 

Let 

\[
M = \begin{bmatrix}
\alpha & \beta\\
\gamma & \delta
\end{bmatrix} \in\textrm{Mat}(2; F)
\]

By lemma \ref{isConj_upper_triangular_iff}, we only need to show that we can find a matrix $C \in \SL_2(F)$ such that when it acts on $M$ by conjugation, the bottom left entry is annihilated.

\begin{itemize}
    \item Suppose on the one hand that $\beta \ne 0$
    
    Observe that 
    
    \begin{equation}\label{triang}
        s_\sigma M s_\sigma^{-1} = \left(\begin{bmatrix}
            -\beta \sigma + \alpha & \beta \\
            -\beta \sigma^{2} + \alpha \sigma - \delta \sigma + \gamma & \beta \sigma + \delta
            \end{bmatrix}\right)
    \end{equation}

    Given $F$ is algebraically closed we can set $\sigma \in F$ to be a root of the polynomial

    \[
    P(X) := -\beta X^{2} + \alpha X - \delta X + \gamma 
    \]

    setting $C := s_\sigma$ yields the desired element which triangularises $M$.
    

    \item Suppose on the other hand that $\beta = 0$
    
    Given the top right entry is zero, we only need find a matrix in $\SL_2(F)$ which flips the anti-diagonal entries (modulo modifying the signs)
    it is thus sufficient to use 
        \[
        w = \begin{bmatrix}
        0 & -1\\
        1 & 0
        \end{bmatrix} \quad \text{as indeed} \quad w M w^{-1} = \begin{bmatrix}
            \delta & -\gamma\\
            0 & \alpha
        \end{bmatrix} \text{ is in triangular form} 
        \]
\end{itemize}

\end{proof}


\begin{corollary}[Upper triangular matrices are conjugate to lower triangular matrices]
    \label{lower_triangular_isConj_upper_triangular}
    \uses{SpecialMatrices.w}
    \lean{lower_triangular_isConj_upper_triangular}
    \leanok
    For every $U \in\textrm{Mat}(2; F)$ that is upper triangular the matrix $w U w^{-1}$ is a lower triangular matrix
\end{corollary}
\begin{proof}
    \leanok
    Direct computation shows this result, see the Lean code!
\end{proof}


\begin{lemma}
    \label{upper_triangular_isConj_diagonal_of_nonzero_det}
    \lean{upper_triangular_isConj_diagonal_of_nonzero_det}
    \leanok
    An upper triangular matrix $U = \begin{bmatrix}
        \alpha & \beta\\
        0 & \delta
    \end{bmatrix}$ is conjugate to a diagonal matrix if $\alpha - \delta \ne 0$
\end{lemma}
\begin{proof}
    \leanok
We show this by direct computation.

Conjugation of $M$ by the matrix 

\[
C := \begin{bmatrix}
    1 & \frac{\beta}{\alpha - \delta}\\
    0 & 1
\end{bmatrix}
\]

yields a diagonal matrix (see the Lean code for the computation!).
\end{proof}


% \begin{remark}[Automation in Lean]
%     Observe that the proof of the theorem above primarily uses tactics! Tactics l 
% \end{remark}


\begin{proposition}
\label{SL2_IsConj_d_or_IsConj_s_or_IsConj_neg_s_of_AlgClosed}
\uses{SpecialMatrices.s, SpecialMatrices.d}
\lean{SL2_IsConj_d_or_IsConj_s_or_IsConj_neg_s_of_AlgClosed}
\leanok
    Each element of $\SL_2(F)$ is conjugate to either $d_\delta$ for some $\delta \in F^\times$, or to $\pm s_\sigma$ for some $\sigma \in F$.
\end{proposition}

\begin{proof}
\uses{isTriangularizable_of_algClosed, lower_triangular_isConj_upper_triangular, upper_triangular_isConj_diagonal_of_nonzero_det}
\leanok Since $F$ is algebraically closed, any element $x \in \SL_2(F)$can be regarded as a linear transformation in the 2 dimensional vector space over $F$, with the eigenvalues $\pi_1$ and $\pi_2$. \\
\\
\space If $\pi_1$ and $\pi_2$ are distinct, then $x$ is thus diagonalisable. That is, there exists an invertible matrix $a \in GL(2, F)$ such that $y = axa^{-1}$ is a diagonal matrix. Furthermore, we can multiply $a$ by a suitable scalar to find an element in $\SL_2(F)$which conjugates $x$ and $y$:

\begin{align*}
    \text{Set} \; b = \frac{a}{\sqrt {\text{det}(a)}}, \quad \text{thus } \; bxb^{-1} =\frac{a}{\sqrt {\text{det}(a)}} \; x \; (\sqrt{\text{det}(a)} \; )\,a^{-1} = axa^{-1} = y.
\end{align*}

Observe that det$(b)=1$, hence $x$ and $y$ are conjugate in $L$. Furthermore, since $y$ is a diagonal matrix it must belong to the set $D$, showing that $x$ is conjugate to $d_\delta$ for some $\delta \in F^\times$. \\
\\
\space If $\pi_1 = \pi_2$ then $x$ has just one repeated eigenvalue. Suppose that $x$ is diagonalisable. Then there exists an element $c \in GL(2, F)$ and a diagonal matrix $\pi_1 I_G$ such that $x = c(\pi_1 I_G)c^{-1} = \pi_1 I_G$. Thus $x = \pm I_G$, which trivially belongs to both $D$ and $Z$. \\
\\
Now assume that $x$ is not diagonalisable. Chapter 7 of \cite{matrix} shows that there exists an element $d \in GL(2, F)$, such that $x= djd^{-1}$, where, $$j = \begin{bmatrix} \pi_1 & 1 \\ 0 & \pi_1 \end{bmatrix}$$ is the Jordan Normal Form of $x$. By the method described above, we can multiply $d$ by a suitable scalar to show that $x$ is conjugate to $j$ in $L$. Now we conjugate $j$ by an element of $\SL_2(F)$whose top left entry is 0.

\begin{align*}
    \begin{bmatrix} 0 & -\gamma^{-1} \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} \pi_1 & 1 \\ 0 & \pi_1 \end{bmatrix} \begin{bmatrix} \delta & \gamma^{-1} \\ -\gamma & 0 \end{bmatrix} = \begin{bmatrix} 0 & -\gamma^{-1} \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} \pi_1 \delta - \gamma & \pi_1 \gamma^{-1} \\ -\pi_1 \gamma & 0 \end{bmatrix} = \begin{bmatrix} \pi_1 & 0 \\ -\gamma^{2} & \pi_1 \end{bmatrix}
\end{align*}
\\
Now clearly the determinant of $x$ is equal to the determinant of $j$, namely 1, which means that $\pi_1 = \pm 1$. This shows that $j$ is conjugate in $\SL_2(F)$to some element in $\times Z$ as well as $x$. Furthermore, since conjugation is transitive, $x$ is conjugate to $\pm s_\sigma$ for some $\sigma \in F$.

\end{proof}


\begin{remark}
formalising the classification of elements of $\SL_2(F)$ up to conjugation in Lean was surprisingly difficult because the informal proof of proposition \ref{SL2_IsConj_d_or_IsConj_s_or_IsConj_neg_s_of_AlgClosed} extracted from Christopher Butler's exposition 
uses the Jordan Normal Form theorem, and at the time of writing, the Jordan Normal form theorem is still not yet in \texttt{mathlib}.

The original approach to formalise the Jordan Normal form theorem for $2 \times 2$ matrices involved studying the eigenspace and generalized eigenspaces of the endomorphism associated to a $2 \times 2$ matrix.
This is one the standard approached taught in an undergraduate curriculum, yet surprisingly, to formalise the $2 \times 2$ case with this approach was rather untractable.

The reason this approach, and often other standard techniques might not integrate well with \texttt{mathlib}, often happens for the following reasons I will now outline.

The crux of formalising a mathematical result always lies at finding the right abstraction, as illustrated in \ref{lattice}, understanding the lattice structure on the set of subgroups becomes an indispensable tool for formalising
results regarding subgroups and their properties. For this particular formalisation, the right abstraction was not entirely.

Is it best to prove the theorem for matrices or for endomoprhisms? Which will be the easiest approach? Which approach is most general? Which approach yields the most amount of useful lemmas?

The reason why the Jordan Normal Form theorem is not yet in \texttt{mathlib} is because it hinges on the following two results which have not been formalised yet:

\begin{enumerate}
    \item The classification of nilpotent endomorphisms.
    \item The classification of semisimple endomorphisms.
\end{enumerate}

Such formalisation would be an amazing project to undertake. But bear in mind, the theorem formalized is the more general Jordan-Chevallier theorem.

To the authors understanding, the general theorem will be formalised by studying the eigenspace and general eigenspace. This approach turned out to be essentially equivalent in 
difficulty to formalising the special case of $2 \times 2$ matrices over an algebraically closed field with the same approach since the argument is inductive on the dimension.

Therefore, after discussions with Prof. Kevin Buzzard's it turned out to be much more effective approach to classify matrices of the special linear group up to 
conjugation by splitting on a few different cases of what a $2 \times 2$ matrix might look like and finding the suitable matrices by which to conjugate to put them in either the form of $d_\delta$ or $\pm s_\sigma$.
\end{remark}

\section{Centralizers \& Normalizers}

Both the centralizer and normalizer of a subset $H$ are subgroups of $G$. Note also that the centralizer is a stronger condition than the 
normalizer and any element in the centralizer of $H$ is also in its normalizer. If $H$ is a singleton then it's clear that its centralizer and normalizer are equal.\\

\subsubsection{Normalizers}

\begin{definition}
The \textbf{normalizer} $N_G(H)$ of a subset $H$ of a group $G$ is the set of elements of $G$ which stabilise $H$ under conjugation.
\begin{equation*} N_G(H) = \{ g \in G : gHg^{-1}=H\}. \end{equation*}
\end{definition}


\begin{corollary}
    \label{lower_triangular_iff_top_right_entry_eq_zero}
    \lean{lower_triangular_iff_top_right_entry_eq_zero}
    \leanok
    A matrix $M \in \textrm{Mat}(2; F)$ is lower triangular if and only if the $M_{12} = 0$.
\end{corollary}
\begin{proof}
    \leanok
It is easy to see the top right entry must be zero for a matrix to be lower triangular.
\end{proof}


\begin{proposition}[Normalizer of subgroups of $S$ are contained in $L$]
\label{normalizer_subgroup_S_le_L}
\uses{SpecialSubgroups.S, SpecialSubgroups.L}
\lean{normalizer_subgroup_S_le_L}
\leanok
 For any subgroup $S_0 \leq S$ with order greater than 1, we have that the normalizer $N_{\SL_2(F)}(S_0) \subset L$.
\end{proposition}
\begin{proof}
    \uses{mem_L_iff_lower_triangular, lower_triangular_iff_top_right_entry_eq_zero}
    \leanok
Let $s_\sigma$ be an arbitary element of $S_0$ with $\sigma \neq 0$. To determine the normalizer of $S_0$ in $\SL_2(F)$we consider which $x \in \SL_2(F)$ satisfy $x s_\sigma x^{-1} \in S_0$.
\begin{align*} x s_\sigma x^{-1} &= \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} 1 & 0 \\ \sigma & 1 \end{bmatrix} \begin{bmatrix} \delta & - \beta \\ - \gamma & \alpha \end{bmatrix}
\\[1.5ex] &= \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} \delta & - \beta \\ \delta \sigma - \gamma & \alpha - \beta \sigma \end{bmatrix}
\\[1.5ex] &= \begin{bmatrix} \alpha \delta - \beta \gamma + \beta \delta \sigma & - \beta^2  \sigma \\ \delta^2 \sigma & \alpha \delta - \beta \gamma - \beta \delta \sigma \end{bmatrix}.
\end{align*}
Since $x s_\sigma x^{-1} \in S_0$ we have $- \beta^2  \sigma = 0$ and since $\sigma \neq 0$, we have $\beta = 0$. Since $s_\sigma$ was chosen arbitrarily, 
any element which normalizes $S_0$ is a lower triangular matrix and is therefore in $L$ by \ref{mem_L_iff_lower_triangular}. Thus $N_{\SL_2(F)}(S_0) \subset L$ as required. \\
\end{proof}


\begin{lemma}
    \label{ex_of_card_D_gt_two}
    \lean{ex_of_card_D_gt_two}
    \leanok
    If the cardinality of finite subgroup of $D_0 \le D$ is greater than $2$ then there exists an element $x \in D_0$ which does not belong to the center $Z$, that is, $x \ne d_1 = I$ and $x \ne d_{-1} = -I$.
\end{lemma}
\begin{proof}
    \leanok
 Suppose for a contradiction that if $\delta \ne \pm 1$ then $d_\delta \notin D_0$. We show that $D_0 \le Z$ and therefore, $|D_0| \le 2$, a contradiction.
 
 Let $d_\delta \in D_0 \le D$ then given $d_\delta \notin D_0$ if $\delta \ne \pm 1$ and $Z = \langle -I\rangle = \{I, -I\}$. It immediately follows that $D_0 \le Z$.

\end{proof}


\begin{proposition}[Normalizers of subgroups of $D$ are contained in $L$]
\label{normalizer_subgroup_D_eq_DW}
\uses{SpecialSubgroups.D, SpecialSubgroups.DW}
\lean{normalizer_subgroup_D_eq_DW}
\leanok
    $N_{\SL_2(F)}(D_0) = \langle D , w \rangle$, where  $D_0$ is any subgroup of $D$ with order greater than 2. \\
    \end{proposition}

\begin{proof}
    \uses{SpecialLinearGroup.fin_two_diagonal_iff, SpecialLinearGroup.fin_two_antidiagonal_iff, ex_of_card_D_gt_two}
    \leanok
    Since $|D_0| > 3$, we can choose a $d_\delta \in D_0 \! \setminus \! Z$, that is where $\delta \neq 1$. To determine the normalizer of $D_0$ in $\SL_2(F)$we consider which $x \in \SL_2(F)$satisfy $x d_\delta x^{-1} \in D_0$.
    \begin{align}\label{6.3proof3} xd_\delta x^{-1} &= \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \begin{bmatrix} \delta & - \beta \\ - \gamma & \alpha \end{bmatrix} \nonumber \\[1.5ex]
    &= \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} \delta \delta & - \beta \delta \\ - \gamma \delta^{-1} & \alpha \delta^{-1} \end{bmatrix} \nonumber \\[1.5ex]
    &= \begin{bmatrix} \alpha \delta \delta - \beta \gamma \delta^{-1} & \alpha \beta (\delta^{-1} - \delta) \\ \gamma \delta (\delta - \delta^{-1}) & \alpha \delta \delta^{-1} - \beta \gamma \delta \end{bmatrix} \in D_0.
    \end{align}
    
    Since (\ref{6.3proof3}) is in $D_0$, the top right and bottom left entries must be 0. Since  $\delta \neq \pm 1$, we have $\delta \neq \delta^{-1}$ and so $\alpha \beta = 0 = \gamma \delta$. \\
    \\
     \space If $\alpha = 0$, then $\beta$ and $\gamma$ are non-zero since det$(x) = 1$, thus $\delta = 0$. So det$(x) = - \gamma \beta = 1$  and $- \gamma = \beta^{-1}$. (\ref{6.3proof3}) becomes $$\begin{bmatrix} \delta^{-1} & 0 \\ 0 & \delta \end{bmatrix} = d^{-1}_\delta.$$Since $D_0$ is a group, it contains the inverse of each of it's elements, so $d^{-1}_\delta \in D_0$ as required. In this case we have $x \in wD$. \\
    \\
     \space If $\alpha \neq 0$, then similarly $\beta = 0$, $\delta = \alpha^{-1}$ and $\gamma = 0$. (\ref{6.3proof3}) now becomes $$\begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} = d_\delta \in D_0.$$This time we have $x \in D$. So $x \in D \cup wD = \langle D , w \rangle$ and any element which normalises $D_0$ is in $\langle D , w \rangle$, thus $N_{\SL_2(F)}(D_0) \subset \langle D , w \rangle$. \\
    \\
    Now take an arbitrary $y \in \langle D , w \rangle = D \cup wD$. If $y \in D$ then $y = d_{\rho 1}$, for some $\rho 1 \in F^\times$.
    \begin{align*} d_{\rho 1} d_\delta d^{-1}_{\rho 1} = d_\delta \in D_0.
    \end{align*}
    
    If $y \in wD$ then $y = w d_{\rho 2}$, for some $ d_{\rho 2} \in F^\times$.
    \begin{align*} (w d_{\rho 2}) d_\delta (w d_{\rho 2})^{-1} &= w d_{\rho 2} d_\delta d^{-1}_{\rho 2} w^{-1}
    \\ &= w d_\delta w^{-1}
    \\ &= d^{-1}_\delta \in D_0.
    \end{align*}
    
    Thus $y$ indeed the whole of $\langle D , w \rangle$ is contained in $N_{\SL_2(F)}(D_0)$. This inclusion gives the desired result, $N_{\SL_2(F)}(D_0) = \langle D , w \rangle$. \\
    
\end{proof}


%-----------------------------

\subsubsection{Centralisers}

\begin{definition}[Centralizer]
The \textbf{centralizer} $C_G(H)$ of a subset $H$ of a group $G$ is the set of elements of $G$ which commute with each element of $H$.
\begin{equation*} 
    C_G(H) = \{ g \in G  : gh=hg, \quad \forall h\in L \}. \end{equation*} 
\end{definition}

\begin{corollary}
    \label{centralizer_neg_eq_centralizer}
    \lean{centralizer_neg_eq_centralizer}
    \leanok
    Let $x \in \SL_2(F)$ then the centralizer of the negative equals $C_{SL_2(F)}(x) = C_{SL_2(F)}(-x)$.
\end{corollary}
\begin{proof}
    \leanok
 An element $y \in \SL_2(F)$ belongs to $C_{SL_2(F)}$ if and only if
  $1 = x y x^{-1} y^{-1} = (-x) y (-x^{-1}) y^{-1}$  if and only if $y$ belongs to $C_{\SL_2(F)}(-x)$.
\end{proof}

    

\begin{proposition}[Centralizer of noncenter $s_\sigma$]
\label{centralizer_s_eq_SZ}
\uses{SpecialSubgroups.S, SpecialSubgroups.Z, SpecialMatrices.s}
\lean{centralizer_s_eq_SZ}
\leanok
The centralizer $C_{\SL_2(F)}(\pm s_\sigma) =  S \times Z $ where $\sigma \neq 0$.
\end{proposition}

\begin{proof}
    \uses{SpecialLinearGroup.fin_two_shear_iff, centralizer_neg_eq_centralizer}
    \leanok
To determine the centralizer of $s_\sigma$ in $L$, we consider which $y \in \SL_2(F)$satisfy $y s_\sigma = s_\sigma y$ for an arbitrarily chosen $s_\sigma$, with $\sigma \neq 0$. \\
\vspace{-0.5mm}
\begin{align}\label{6.3proof2} y s_\sigma &= s_\sigma y, \nonumber \\[1.5ex]
\begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} 1 & 0 \\ \sigma & 1 \end{bmatrix} &= \begin{bmatrix} 1 & 0 \\ \sigma & 1 \end{bmatrix} \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix}, \nonumber \\[1.5ex]
\begin{bmatrix} \alpha + \beta \sigma & \beta \\ \gamma + \delta \sigma & \delta \end{bmatrix} &= \begin{bmatrix} \alpha & \beta \\ \gamma +  \alpha \sigma & \delta + \beta \sigma \end{bmatrix}.
\end{align}
\vspace{.5mm}

Equating the top left entries of (\ref{6.3proof2}) gives $\alpha + \beta \sigma = \alpha$ which means $\beta = 0$ since $\sigma \neq 0$ by assumption. Equating the bottom left entries gives that $\alpha = \delta$. Finally, since det$(y) = 1$, we have $\alpha \delta = 1$ so $\alpha = \pm 1$. Thus a $y \in C_{\SL_2(F)}(s_\sigma)$ is

\begin{align*} y &= \begin{bmatrix} \alpha & 0 \\ \gamma & \alpha \end{bmatrix}. \tag{where $\alpha = \pm 1$}
\end{align*}

So $y = \pm s_\sigma$ for some $\sigma \in F$, and $SZ = \{ \pm s_\sigma \} \subset C_{\SL_2(F)}(s_\sigma)$. Now take an arbitrary $s_\gamma z \in SZ$.
\begin{align*} (s_\gamma z) s_\sigma &= s_\sigma (s_\gamma z),
\\ s_\gamma s_\sigma z &= s_\sigma s_\gamma z, \tag{since $z \in Z$}
\\ s_{\gamma + \sigma} &= s_{\gamma + \sigma}.
\end{align*}

Thus $s_\gamma z$ and indeed the whole of $SZ$ is contained in $ C_{\SL_2(F)}(s_\sigma)$, so $C_{\SL_2(F)}(s_\sigma) = SZ$. \\
\\
Since $S$ commutes elementwise with $Z$ and $S \cap Z = \{ I_G \}$, we can apply Corollary \ref{directproductZ} and assert that $C_{\SL_2(F)}(s_\sigma) = SZ \cong S \times Z$ as required. The centralizer of $- s_\sigma$ is also $ S\times Z$, 
since an element $x$ commutes with $- s_\sigma$ if and only if it commutes with $s_\sigma$:
\begin{align*} 
    xs_\sigma = s_\sigma x \iff -(x s_\sigma) = - (s_\sigma x) \iff x(- s_\sigma) = (- s_\sigma)x.
\end{align*}

Note that in case of $\sigma = 0$, $\pm s_\sigma \in Z$ and thus it's centralizer is the whole of $L$.

\end{proof}


    

\begin{proposition}[Centralizer of noncenter $d_\delta$]
    \label{centralizer_d_eq_D}
    \uses{SpecialMatrices.d, SpecialSubgroups.D}
    \lean{centralizer_d_eq_D}
    \leanok
    The centralizer $C_{\SL_2(F)}(d_\delta) = D$ for $\delta \neq \pm 1$.
    \end{proposition}        
\begin{proof}
    \uses{SpecialLinearGroup.fin_two_diagonal_iff}
    \leanok
Now we consider which $y \in \SL_2(F)$satisfy $y d_\delta = d_\delta y$ for an arbitrarily chosen $d_\delta$, with $\delta \neq \pm 1$.
\begin{align}\label{6.3proof4} y d_ \delta &= d_\sigma y, \nonumber \\[1.5ex]
\begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} &= \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix}, \nonumber \\[1.5ex]
\begin{bmatrix} \alpha \delta & \beta \delta^{-1} \\ \gamma \delta & \delta \delta^{-1} \end{bmatrix} &= \begin{bmatrix} \alpha \delta & \beta \delta \\ \gamma \delta^{-1} & \delta \delta^{-1} \end{bmatrix}.
\end{align}

Equating the top right and bottom left entries of (\ref{6.3proof4}) gives that $\beta = 0 = \gamma$ since Since $\delta \neq \delta^{-1}$. Thus $\delta = \alpha^{-1}$ and 
\begin{align*} x = \begin{bmatrix} \alpha & 0 \\ 0 & \alpha^{-1} \end{bmatrix} \in D. 
\end{align*}

Thus $x$ and indeed the whole of $C_{\SL_2(F)}(d_\delta)$ is contained in $D$. Now take an arbitrary $d_\rho \in D$.
\begin{align*} d_\rho d_\delta = d_{\rho \delta} = d_\delta d_\rho.
\end{align*}
So clearly $D \subset C_{\SL_2(F)}(d_\delta)$ and thus $C_{\SL_2(F)}(d_\delta) = D$ as required.
\end{proof}


%---------------------------------------------------


\begin{proposition}[Centralizers of conjugate elements]
    \label{conjugate_centralizers_of_IsConj}
    \lean{conjugate_centralizers_of_IsConj}
    \leanok
    Let $a$ and $b$ be conjugate elements in a group $G$. Then $\exists \, x \in G$ such that $xC_G(a)x^{-1} = C_G(b)$. \vspace{3mm}
\end{proposition}
\begin{proof}
\leanok
This proposition essentially claims that conjugate elements have conjugate centralizers. Since $a$ and $b$ are conjugate there exists an $x \! \in \! G$ such that $b = xax^{-1}$. Let $g$ be an arbitrary element of $C_G(a)$. Then,

\begin{align*} (xgx^{-1})(xax^{-1}) &= xgax^{-1}\\
&= xagx^{-1} \tag{since $g \in C_G(a)$}\\
&= (xax^{-1})(xgx^{-1}). \end{align*}

Thus $xgx^{-1} \in C_G(xax^{-1})$. Since $g$ was chosen arbitrarily, $$xC_G(a)x^{-1} \subset C_G(xax^{-1}) = C_G(b).$$ 

Conversely, let $h$ be an arbitary element of $C_G(xax^{-1})$. Then,

\begin{align*} (x^{-1}hx)a &= x^{-1}h(xax^{-1})x \\
&= x^{-1}(xax^{-1})hx \tag{since $h \in C_G(xax^{-1})$} \\
&= a(x^{-1}hx). \end{align*}

So $x^{-1}hx \in C_G(a)$ and since $h$ was arbitrarily chosen from $C_G(xax^{-1})$, \linebreak $x^{-1}C_G(xax^{-1})x \subset C_G(a)$. Multiplication on the left by $x$ and on the right by $x^{-1}$ gives $C_G(b) =  C_G(xax^{-1}) \subset xC_G(a)x^{-1}$. Since we have shown that each set contains the other, $xC_G(a)x^{-1} = C_G(b)$ as required. \\
\end{proof}




\begin{corollary}[ Centralizer of non-central element is commutative]
    \label{IsCommutative_centralizer_of_not_mem_center}
    \lean{IsCommutative_centralizer_of_not_mem_center}
    \leanok
The centralizer of an element $x$ in $\SL_2(F)$ is abelian unless $x$ belongs to the centre of $L$. \vspace{3mm}
\end{corollary}

\begin{proof}
    \uses{SL2_IsConj_d_or_IsConj_s_or_IsConj_neg_s_of_AlgClosed, conjugate_centralizers_of_IsConj, centralizer_s_eq_SZ, centralizer_d_eq_D}
    \leanok
    This is almost an immediate consequence of the preceding results. Propositions \ref{centralizer_s_eq_SZ} and \ref{centralizer_d_eq_D} show that an element of the form $\pm s_\sigma$ which does not lie in the centre of $\SL_2(F)$ has centralizer $S \times Z$, whilst a non-central element of the form $d_\delta$ has centralizer $D$.
Both $S$ and $D$ are abelian since they are isomorphic to $F$ and $F^\times$ respectively. Let $s_\sigma z_1$ and $s_\gamma z_2$  be arbitrary elements of $S \times Z$.

\vspace{-.5mm}
\begin{align*} 
    (s_\sigma z_1)(s_\gamma z_2)  &= s_\sigma s_\gamma z_2 z_1  \tag{since  $z_1 \in Z$}
\\ &= s_\gamma s_\sigma z_2 z_1  \tag{since  $S$ is abelian}
\\ &= (s_\gamma z_2)(s_\sigma z_1).   \tag{since  $z_2 \in Z$}
\end{align*} 

Thus $S \times Z$ is also abelian. Since every element of $\SL_2(F)$ is conjugate to $d_\delta$ or $\pm s_\sigma$ by Proposition \ref{SL2_IsConj_d_or_IsConj_s_or_IsConj_neg_s_of_AlgClosed} and conjugate elements have conjugate centralizers by Proposition \ref{conjugate_centralizers_of_IsConj}, the centralizer of each $x \in \SL_2(F)\setminus Z$ is conjugate to either $\times Z$ or $D$. 
Since conjugate subgroups are isomorphic, they must have the same structure, and thus since both $S \times Z$ and $D$ are abelian, $C_{\SL_2(F)}(x)$ is also abelian. 
Note that in general this does hold for $x \in Z$, since its centralizer is the whole of $\SL_2(F)$ which is not abelian unless $\SL_2(F)= Z$.

\end{proof}




\section{The Projective Line \& Triple Transitivity}

It is convenient to sometimes take a geometric viewpoint and regard the elements of $\SL_2(F)$as pairs of vectors in the 2-dimensional vector space over $F$, which we will denote $V$. An element of $\SL_2(F)$is thus a linear transformation of $V$. 

\begin{definition} 
    Let $\mathscr{L}$ be the set of all 1-dimensional subspaces of $V$. A subset $\mathscr{S}$ of $\mathscr{L}$ is called a \textbf{subspace} of $\mathscr{L}$ if there is a subspace $U$ of $V$ such that $\mathscr{S}$ is the set of all 1-dimensional spaces of $U$. We have dim $U =$ dim $\mathscr{S} + 1$. The set $\mathscr{L}$ on which this concept of subspaces is defined is called the \textbf{projective line} on $V$ and an element of $\mathscr{L}$ is a 0-dimensional subspace of $\mathscr{L}$ and consequently called a \textbf{point}. The projective line can be considered as a straight line in the field, plus a point at infinity.
\end{definition}

Any 1-dimensional subspace of $V$ is a set of vectors of the form $\eta u$, where $u$ is a non-zero vector of $V$ and $\eta \in F^\times$. Thus the points of $\mathscr{L}$ are equivalence classes with the following relation defined on the set of vectors of $V$.
\begin{align*} u = \begin{bmatrix} u_1 \\ u_2 \end{bmatrix} \sim \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = v \iff u = \eta v, \qquad (\text{for $\eta \in F^\times$}).
\end{align*}

Notice that $u$ and $v$ are equivalent if and only if $u_1 v_2 = v_1 u_2$. Importantly each point $P_i$ of $\mathscr{L}$ can be represented by a corresponding equivalence class of vectors of $V$, that is, $P$ corresponds to $u$ if $P = u_1 / u_2$. In the case when $u_2 = 0$, this corresponds to the point at infinity.

\begin{definition} Let $S$ be a permutation group which acts on a set $X$ and $\{ x_1, x_2, x_3 \}$ and $\{ x_1', x_2', x_3' \}$ be two subsets of distinct elements of $X$. Then $S$ is said be \textbf{triply transitive} on $X$ if there is an element $\pi \in S$ such that,
\begin{align*} x^{\pi}_i = x'_i, \qquad(\text{$i$ = 1,2 or 3}).
\end{align*} 
\end{definition}

\begin{theorem} \label{6.6}
Let $\mathscr{L}$ be the projective line over the field $F$. Then $\SL_2(F)$is triply transitive on the set of the points of $\mathscr{L}$. \vspace{3mm}
\end{theorem}

\begin{proof} Let $P_1$, $P_2$ and $P_3$ be distinct points of $\mathscr{L}$ and $p_i$ be a vector in $V$ corresponding to $P_i$. Since each $P_i$ is distinct, $p_1$, $p_2$ and $p_3$ are thus pairwise linearly independent. Thus $p_1$ and $p_2$  form a basis for $V$ and it's clear that there exist $\alpha, \beta \in F^\times$ such that,
\begin{align*} p_3 = \alpha p_1 + \beta p_2.
\end{align*}

Now, let $Q_1$, $Q_2$ and $Q_3$ be three more distinct points of $\mathscr{L}$ and $q_i$ be a vector in $V$ corresponding to $Q_i$. Similarly, by the above argument, there exist $\gamma, \delta \in F^\times$ such that,
\begin{align*} q_3 = \gamma q_1 + \delta q_2.
\end{align*}

Let $\pi \in GL(2,F)$ be the linear transformation which sends $\alpha p_1$ to $\gamma q_1$  and $\beta p_2$ to $\delta q_2$. Thus,
\begin{align*} \pi(p_3) = \pi(\alpha p_1 + \beta p_2) = \pi(\alpha p_1) + \pi(\beta p_2) = \gamma q_1 + \delta q_2 = q_3 
\end{align*}

Hence we get $P^\pi_1 = Q_1$, $P^\pi_2 = Q_2$ and $P^\pi_3 = Q_3$ and $GL(2,F)$ is triply transitive. Now set,
\begin{align*} \eta = \sqrt{\frac{1}{\text{det }\pi}}.
\end{align*}

Consider the mapping $\theta$ which sends $\alpha p_1$ to $\eta \gamma q_1$ and $\beta p_2$ to $\eta \delta q_2$. Observe that,
\begin{align*} \text{det }\theta = \eta^2 \, \text{det } \pi = 1
\end{align*}

So $\theta \in SL(2,F) = \SL_2(F)$and since $P^\theta_1 = Q_1$, $P^\theta_2 = Q_2$ and $P^\theta_3 = Q_3$, we have that $\SL_2(F)$is also triply transitive. 

\end{proof}

The following proposition looks at what happens when the group $\SL_2(F)$acts on the projective line $\mathscr{L}$.

\begin{proposition} \label{6.7} (i) Each element of the form $d_\delta$ (with $\delta \neq \pm 1$), fixes the same two points on the projective line $\mathscr{L}$ and fix no other point. \vspace{3mm} \\
(ii) Each element of the form $\pm s_\sigma$ (with $\sigma \neq 0$), fixes the same point $P$ on $\mathscr{L}$ and fix no other point. Furthermore, \emph{Stab}$(P) = H$. \vspace{3mm} \\
(iii) All conjugate elements have the same number of fixed points on $\mathscr{L}$. \vspace{3mm} \\
(iv) Any noncentral element of $\SL_2(F)$has at most 2 fixed points on $\mathscr{L}$.
\end{proposition}

\begin{proof} 
(i) Let $P$ be a fixed a point of an arbitrary $d_\delta \in D$, with $\delta \neq \pm 1$ and let $u$ belong to the corresponding equivalence class of vectors of $V$ to $P$. \\
\begin{align*} d_\delta u = \begin{bmatrix} \delta & 0 \\ 0 & \delta^{-1} \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \end{bmatrix} &= \begin{bmatrix} u_1 \delta \\ u_2 \delta^{-1} \end{bmatrix} \sim \begin{bmatrix} u_1 \\ u_2 \end{bmatrix}, 
\\[1.5ex] u_1 u_2 \delta &= u_1 u_2 \delta^{-1}.
\end{align*}

Since $\delta \neq \pm 1$, $\delta$ does not equal $\delta^{-1}$, and so either $u_1 = 0$ or $u_2 = 0$. Thus $u$ is equivalent to either the vector $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ or $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and these correspond to 2 distinct points of $\mathscr{L}$ which are fixed by $d_\delta$. \\
\\
(ii) Let $P$ be a fixed a point of an arbitrary $s_\sigma$, with $\sigma \neq 0$, and let $u$ be the corresponding element of $V$ to $P$. \\
\begin{align*} s_\sigma u = \begin{bmatrix} 1 & 0 \\ \sigma & 1 \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \end{bmatrix} &= \begin{bmatrix} u_1 \\ u_1 \sigma + u_2 \end{bmatrix} \sim \begin{bmatrix} u_1 \\ u_2 \end{bmatrix}, 
\\[1.5ex] u_1 u_2 &= {u_1}^2 \sigma + u_1 u_2.
\end{align*}

This gives ${u_1}^2 \sigma = 0$ and since $\sigma \neq 0$ we have $u_1 = 0$. Thus $s_\sigma$ has just one fixed point, $P$ which corresponds to the equivalence class of $\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ in $V$. We show also that $P$ is also the only fixed point of $-s_\sigma$, with $\sigma \neq 0$.
\begin{align*} -s_\sigma u = \begin{bmatrix} -1 & 0 \\ \sigma & -1 \end{bmatrix} \begin{bmatrix} u_1 \\ u_2 \end{bmatrix} &= \begin{bmatrix} -u_1 \\ u_1 \sigma - u_2 \end{bmatrix} \sim \begin{bmatrix} u_1 \\ u_2 \end{bmatrix}, 
\\[1.5ex] -u_1 u_2 &= {u_1}^2 \sigma - u_1 u_2.
\end{align*}

So again $u_1 =0$ and $-s_\sigma$ fixes $P$ and no other point. We now calculate the stabiliser of $P$ in $L$, by considering which $x \in \SL_2(F)$fix $P$. \\
\begin{align*} x u = \begin{bmatrix} \alpha & \beta \\ \gamma & \delta \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} &= \begin{bmatrix} \beta \\ \delta \end{bmatrix} \sim \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
\end{align*}

Thus $\beta = 0$ and $x \in H$. Since $x$ was chosen arbitrarily from Stab$(P)$, we have Stab$(P) \subset H$. Now let an arbitrarily chosen $y \in H$ act on $P$. \\
\begin{align*} y u = \begin{bmatrix} \alpha & 0 \\ \gamma & \alpha^{-1} \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} &= \begin{bmatrix} 0 \\ \alpha^{-1} \end{bmatrix} \sim \begin{bmatrix} 0 \\ 1 \end{bmatrix}.
\end{align*}

Thus $y$ and indeed $H$ is contained in Stab$(P)$, so Stab$(P) = H$ as desired. \\
\\
(iii) Let $P_i$ $(i = 1,2,...)$ be the fixed points of $x\in \SL_2(F)$and let $y$ be conjugate to $x$ in $L$. That is, there exists a $g \in \SL_2(F)$such that $x = gyg^{-1}$.
\begin{align*} x P_i &= P_i,
\\ gyg^{-1} P_i &= P_i,
\\ y(g^{-1} P_i) &= (g^{-1} P_i).
\end{align*}

This shows that $P_i$ is a fixed point of $x$ if and only if $g^{-1} P_i$ is a fixed point of $y$. Thus conjugate elements have the same number of fixed points. \\
\\
(iv) By Proposition \ref{ISL2_IsConj_d_or_IsConj_s_or_IsConj_neg_s_of_AlgClosed} every  element of $\SL_2(F)$is conjugate to either $d_\delta$ or $\pm s_\sigma$, so since conjugate elements have the same number of fixed points, every element of $\SL_2(F)\! \setminus \! Z$ has either the same number of fixed points as $d_\delta$ (with $\delta \neq \pm 1$), namely 2, or the same number as $\pm s_\sigma$, (with $\sigma \neq 0$), namely 1.

\end{proof}


